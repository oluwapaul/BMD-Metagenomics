{\rtf1\ansi\ansicpg1252\cocoartf2709
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 title: "Bacterial growth rate estimation"\
author: "Julian Trachsel"\
date: "October 29, 2019"\
output: \
  html_document:\
    toc: true # table of content true\
    toc_float: true\
    toc_depth: 3  # upto three depths of headings (specified by #, ## and ###)\
    number_sections: true  ## if you want number sections at each table header\
    theme: united  # many options for theme, this one is my favorite.\
    highlight: tango  # specifies the syntax highlighting style\
---\
\
# Purpose  \
\
The purpose of this analysis is to estimate the growth rates of bacterial species in the cecal microbiomes of turkey's fed various amounts of BMD (bacitracin methylene disalicylate)  \
\
## How to estimate bacterial growth rate  \
\
Taken from "Measurement of bacterial replication rates in microbial communities"  \
Christopher T Brown, Matthew R Olm, Brian C Thomas & Jillian F Banfield  \
Nature Biotechnology volume34, pages1256\'961263 (2016)  \
\
>Dividing cells in a natural population contain, on average, more than one copy of their genome. In an unsynchronized population of growing bacteria, cells contain genomes that are replicated to different extents, resulting in a gradual reduction in the average genome copy number from the origin to the terminus of replication. This decrease can be detected by measuring changes in DNA sequencing coverage across complete genomes. Bacterial genome replication proceeds bi-directionally from a single origin of replication, therefore the origin and terminus of replication can be deduced based on this coverage pattern. GC skew and genome coverage analyses of a wide variety of bacteria have shown that this replication mechanism is broadly applicable. Further, early studies of bacterial cultures revealed that cells can achieve faster division by simultaneously initiating multiple rounds of genome replication, which results in an average of more than two genome copies in rapidly growing cells.  \
  \
  \
![iRep_explain](./iRep_explain.JPG)  \
\
\
> *(a)* Populations of bacteria undergoing rapid cell division differ from slowly growing populations in that the individual cells of a growing population are more actively in the process of replicating their genomes (purple circles). *(b)* Differences in genome copy number across a population of replicating cells can be determined based on sequencing read coverage over complete genome sequences. The ratio between the coverage at the origin (\'93peak\'94) and terminus (\'93trough\'94) of replication (PTR) relates to the replication rate of the population. The origin and terminus can be determined based on cumulative GC skew. *(c,d)* If no complete genome sequence is available, it is possible to calculate the replication rate based on the distribution of coverage values across a draft-quality genome using the iRep method. Coverage is first calculated across overlapping segments of genome fragments. Growing populations will have a wider distribution of coverage values compared with stable populations (histograms). These values are ordered from lowest to highest, and linear regression is used to evaluate the coverage distribution across the genome in order to determine the coverage values associated with the origin and terminus of replication. iRep is calculated as the ratio of these values. *(e)* Genome-resolved metagenomics involves DNA extraction from a microbiome sample followed by DNA sequencing, assembly, and genome binning. Binning is the grouping together of assembled genome fragments that originated from the same genome. This can be done based on shared characteristics of each fragment, such as sequence composition, taxonomic affiliation, or abundance.  \
\
\
# General workflow  \
\
1. Assemble shotgun metagenome  \
2. Bin metagenomic assembly into discrete genomes  \
3. Calculate coverage patterns in these bins\
4. Infer growth rate from coverage patterns  \
\
## Experimental design\
\
![exp_design](./BMD_exp_design.jpg)\
  \
* Collected cecal contents at necropsy  \
* 10 birds per treatment per timepoint = 90 samples total  \
* No repeated measurements, (cecal contents are a terminal sample)  \
* bulk DNA extraction  \
  \
  \
## Data Description  \
\
* HiSeq 2x150 PE reads  \
    1 library per bird = 90 libraries  \
    2,480,187,759 paired reads  \
    avg insert = 280 bp  \
  \
* PacBio RSII reads  \
    library preps from pooled samples  \
    1 library prep per timepoint  \
    equal pool of all treatments  \
    3 PacBio RSII library preps  \
    3 SMRT cells?  \
    7,178,582 total reads\
    Not as long as I was expecting\
\
        \
```\{r, echo=FALSE, message=FALSE\}\
\
library(tidyverse)\
hi <- read_tsv('ALL_pacbio_histogram.txt', skip = 9)\
\
\
hi %>% \
  ggplot(aes(x=`#Length`, y=reads)) + geom_col() + theme_bw() + \
  ggtitle('histogram of pacbio read lengths')\
\
\
hi %>% filter(`#Length` <5000) %>% \
  ggplot(aes(x=`#Length`, y=reads)) + geom_col() + theme_bw()+ \
  ggtitle('histogram of pacbio read lengths less than 5kb')\
\
```\
\
\
\
## Metagenomic Assembly  \
\
* One large co-assembly with all data combined.  \
* bbtools used to QC and normalize reads  \
* Metaspades used for hybrid assembly  \
    Standard short-read assembly, then long reads aligned to assembly graph  \
    \
### Read QC  \
\
\
* The script ALL_QC.SLURM describes the pre-processing steps for the hiseq reads  \
    1. remove optical duplicates  \
    2. remove low quality regions on the flow cell  \
    3. trim adapters  \
    4. remove artifacts (phiX etc.)  \
    5. remove turkey reads  \
    6. error correct  \
    7. normalize  \
  \
  \
```\{bash, eval=FALSE\}\
#!/bin/bash\
\
#SBATCH --job-name="ALL_QC"                            # name of the job submitted\
#SBATCH -p mem                                         # name of the queue you are submitting to\
#SBATCH -n 40                                          # number of cores/tasks in this job, you get all 20 cores with 2 threads per core with hyperthreading\
#SBATCH -N 1                                           # nodes\
#SBATCH --mem=1000G                                    # memory allocation       \
#SBATCH -t 72:00:00                                    # time allocated for this job hours:mins:seconds\
#SBATCH --mail-user=XXXXXXX            # will receive an email when job starts, ends or fails\
#SBATCH --mail-type=BEGIN,END,FAIL                     # will receive an email when job starts, ends or fails\
#SBATCH -o "stdout.%j.%N"                              # standard out %j adds job number to outputfile name and %N adds the node name\
#SBATCH -e "stderr.%j.%N"                              # optional but it prints our standard error\
\
# ENTER COMMANDS HERE:\
\
module load java/1.8.0_121\
\
set -e # I think this means if anything fails it immediately exits and doesnt keep plowing ahead.\
\
#Written by Brian Bushnell\
#Last updated March 4, 2019\
\
#This script is designed to preprocess data for assembly of overlapping 2x150bp reads from Illumina HiSeq 2500.\
#Some numbers and steps may need adjustment for different data types or file paths.\
#For large genomes, tadpole and bbmerge (during the "Merge" phase) may need the flag "prefilter=2" to avoid running out of memory.\
#"prefilter" makes these take twice as long though so don't use it if you have enough memory.\
#The "rm ALL_temp.fq.gz; ln -s reads.fq.gz ALL_temp.fq.gz" is not necessary but added so that any pipeline stage can be easily disabled,\
#without affecting the input file name of the next stage.\
\
#interleave reads\
#reformat.sh in=ALL_R#.fastq.gz out=ALL.fq.gz\
\
# 92 fecal shotgun metagenomic libraries\
# sequenced on HiSeq3000 2x150 PE\
# I think avg insert sizes is ~280?\
\
# this takes the R1 and R2 files and creates interleaved fastq.gz files for each\
for x in *_R1.fastq.gz \
do\
#echo "$\{x%_R1*\}_R2.fastq.gz"\
reformat.sh in1="$x" in2="$\{x%_R1*\}_R2.fastq.gz" out="$\{x%_R1*\}.fq.gz"\
done\
\
# combines all interleaved reads into one file\
cat *.fq.gz > ALL.fq.gz\
\
#Link the interleaved input file as "ALL_temp.fq.gz"\
ln -s ALL.fq.gz ALL_temp.fq.gz\
\
# --- Preprocessing ---\
\
#Remove optical duplicates\
clumpify.sh in=ALL_temp.fq.gz out=ALL.clumped.fq.gz dedupe optical\
rm ALL_temp.fq.gz; ln -s ALL.clumped.fq.gz ALL_temp.fq.gz\
\
#Remove low-quality regions\
filterbytile.sh in=ALL_temp.fq.gz out=ALL.filtered_by_tile.fq.gz\
rm ALL_temp.fq.gz; ln -s ALL.filtered_by_tile.fq.gz ALL_temp.fq.gz\
\
#Trim adapters.  Optionally, reads with Ns can be discarded by adding "maxns=0" and reads with really low average quality can be discarded with "maq=8".\
bbduk.sh in=ALL_temp.fq.gz out=ALL.trimmed.fq.gz ktrim=r k=23 mink=11 hdist=1 tbo tpe minlen=70 ref=adapters ftm=5 ordered\
rm ALL_temp.fq.gz; ln -s ALL.trimmed.fq.gz ALL_temp.fq.gz\
\
#Remove synthetic artifacts and spike-ins by kmer-matching.\
bbduk.sh in=ALL_temp.fq.gz out=ALL.filtered.fq.gz k=31 ref=artifacts,phix ordered cardinality\
rm ALL_temp.fq.gz; ln -s ALL.filtered.fq.gz ALL_temp.fq.gz\
\
#Decontamination by mapping can be done here.\
#JGI removes these in two phases:\
#1) common microbial contaminants (E.coli, Pseudomonas, Delftia, others)\
#2) common animal contaminants (Human, cat, dog, mouse)\
\
bbsplit.sh in=SAMPLE_temp.fq.gz ref=turkey.fa basename=SAMPLE_out_%.fq.gz outu=SAMPLE_clean.fq.gz int=t\
rm SAMPLE_temp.fq.gz; ln -s SAMPLE_clean.fq.gz SAMPLE_temp.fq.gz\
\
\
#Error-correct phase 1\
bbmerge.sh in=ALL_temp.fq.gz out=ALL.ecco.fq.gz ecco mix vstrict ordered ihist=ihist_merge1.txt\
rm ALL_temp.fq.gz; ln -s ALL.ecco.fq.gz ALL_temp.fq.gz\
\
#Error-correct phase 2\
clumpify.sh in=ALL_temp.fq.gz out=ALL.eccc.fq.gz ecc passes=4 reorder\
rm ALL_temp.fq.gz; ln -s ALL.eccc.fq.gz ALL_temp.fq.gz\
\
#Error-correct phase 3\
#Low-depth reads can be discarded here with the "tossjunk", "tossdepth", or "tossuncorrectable" flags.\
#For very large datasets, "prefilter=1" or "prefilter=2" can be added to conserve memory.\
#Alternatively, bbcms.sh can be used if Tadpole still runs out of memory.\
tadpole.sh in=ALL_temp.fq.gz out=ALL.ecct.fq.gz ecc k=62 ordered tossjunk prefilter=1\
rm ALL_temp.fq.gz; ln -s ALL.ecct.fq.gz ALL_temp.fq.gz\
\
#Normalize\
#This phase can be very beneficial for data with uneven coverage like metagenomes, MDA-amplified single cells, and RNA-seq, but is not usually recommended for isolate DNA.\
#So normally, this stage should be commented out, as it is here.\
bbnorm.sh in=ALL_temp.fq.gz out=normalized.fq.gz target=100 hist=khist.txt peaks=peaks.txt\
rm ALL_temp.fq.gz; ln -s normalized.fq.gz ALL_temp.fq.gz\
    \
```\
\
### Assembly with metaspades  \
  \
This ran for about 50 hours.  \
\
\
```\{bash, eval=FALSE\}\
\
#!/bin/bash\
\
#SBATCH --job-name="metaspades"                         # name of the job submitted\
#SBATCH -p mem                                          # name of the queue you are submitting to\
#SBATCH -n 100                                          # number of cores/tasks in this job, you get all 20 cores with 2 threads per core with hyperthreading\
#SBATCH -N 1                                            # number of nodes\
#SBATCH --mem=1400G                                     # memory allocation\
#SBATCH -t 167:00:00                                    # time allocated for this job hours:mins:seconds\
#SBATCH --mail-user=YOUREMAILHERE@email.com             # will receive an email when job starts, ends or fails\
#SBATCH --mail-type=BEGIN,END,FAIL                      # will receive an email when job starts, ends or fails\
#SBATCH -o "stdout.%j.%N"                               # standard out %j adds job number to outputfile name and %N adds the node name\
#SBATCH -e "stderr.%j.%N"                               # optional but it prints our standard error\
\
\
module load java/1.8.0_121\
\
# convert from interleaved to R1 R2 files\
reformat.sh in=ALL_normalized.fq.gz out1=ALL_normalized_1.fq.gz out2=ALL_normalized_2.fq.gz\
\
metaspades.py -o run1 -1 ALL_normalized_1.fq.gz -2 ALL_normalized_2.fq.gz --pacbio ALL_pacbio.fq.gz --only-assembler -m 1400 --tmp-dir "$TMPDIR" -t 100 -k 25,55,95,125\
\
```\
\
### Assembly statistics  \
\
```\{r setup, include=FALSE\}\
# knitr::opts_chunk$set(echo = FALSE)\
# setwd('~/Torey/')\
library(knitr)\
library(kableExtra)\
library(tidyverse)\
\
megahit_maps <- read_tsv('./assembly_stats/megahit500__mapstats.tsv', col_names = c('sample', 'read1', 'read2')) %>% na.omit() %>% mutate(type='megahit')\
\
metaspades_maps <- read_tsv('./assembly_stats/metaspades500_mapstats.tsv', col_names = c('sample', 'read1', 'read2')) %>% na.omit()%>% mutate(type='metaspades')\
\
\
all_map <- rbind(megahit_maps, metaspades_maps)\
\
all_map$day <- sub('(d[0-9]+)([a-z]+)([0-9]+)','\\\\1',all_map$sample)\
all_map$treat <- sub('(d[0-9]+)([a-z]+)([0-9]+)','\\\\2',all_map$sample)\
all_map$animal <- as.numeric(sub('(d[0-9]+)([a-z]+)([0-9]+)','\\\\3',all_map$sample))\
\
all_map <- all_map %>% na.omit()\
\
all_map_gather <- all_map %>% gather(-sample, -day, -treat, -animal, -type, key='read', value='percent_mapped')\
\
test <- read_delim('./assembly_stats/test.txt', delim = '\\t', col_types = 'ccccc')\
\
detailed_stats <- read_delim('./assembly_stats/detailed_stats_comp.txt', delim = '\\t')\
\
\
```\
***  \
\
\
***  \
\
* Comparison of full assemblies  \
* contigs less than 500 bp removed  \
\
```\{r, echo=FALSE\}\
kable(detailed_stats, align = c('l','c','c')) %>% \
  row_spec(0, bold = T) %>% \
  kable_styling(bootstrap_options = c("striped")) \
```\
\
* **Comparisons at different minimum contig length thresholds**  \
\
\
```\{r, echo=FALSE\}\
kable(test, align = c('lcccc'), col.names = c('min scaffold length', 'Megahit','Metaspades', 'Megahit','Metaspades')) %>% \
  row_spec(0, bold = T) %>%\
  add_header_above(c(" ", "Number of Contigs" = 2, "Length of All Contigs" = 2)) %>% \
  kable_styling(full_width = F, bootstrap_options = c("striped")) %>% \
  column_spec(column = c(2,4), background = '#DEEBF7') %>% column_spec(column = c(3,5), background = "#E5F5E0")\
\
\
\
```\
\
\
\
* **What proportion of reads map to the assembly?**  \
\
\
```\{r echo=FALSE\}\
\
\
all_map %>% ggplot(aes(x=read1, y=read2, color=type)) + geom_smooth(method = 'lm', alpha=.3)+ geom_point() + ggtitle('No drastic differences in mapping between read 1 and read 2') + xlab('Percent read 1 mapping') + ylab('percent read 2 mapping')\
\
# all_map_gather %>% ggplot(aes(x=read, y=percent_mapped)) + geom_boxplot() + geom_jitter() + ggtitle('percent reads mapped to megahit assembly by read')\
# \
# all_map_gather %>% ggplot(aes(x=treat, y=percent_mapped)) + geom_boxplot() + geom_jitter()+ ggtitle('percent reads mapped to megahit assembly by treatment')\
# \
# all_map_gather %>% ggplot(aes(x=day, y=percent_mapped)) + geom_boxplot() + geom_jitter() + ggtitle('percent reads mapped to megahit assembly by day')\
\
\
all_map_gather %>% ggplot(aes(x=day, y=percent_mapped, fill=treat)) + geom_boxplot(outlier.colour = NA) + geom_jitter(alpha=.3) + ggtitle('percent reads mapped to megahit assembly by day') + facet_wrap(~type)\
\
```\
\
## Genome binning using Metabat2  \
\
* Basic QC on reads  \
* Map reads from each sample to Metaspades co-assembly  \
* input indexed bam files to metabat2 for binning\
  \
  \
### Read QC  \
  \
  \
* This is a template I used to QC the reads prior to mapping:  \
* script is named 'pre_mapping_QC.SLURM'\
\
```\{bash, eval=FALSE\}\
#!/bin/bash\
\
#SBATCH --job-name="SAMPLE"                            # name of the job submitted\
#SBATCH -p brief-low                                   # name of the queue you are submitting to\
#SBATCH -n 30                                          # number of cores/tasks in this job, you get all 20 cores with 2 threads per core with hyperthreading\
#SBATCH -N 1                                           # number of nodes\
#SBATCH --mem=300G                                     # memory allocation \
#SBATCH -t 2:00:00                                     # time allocated for this job hours:mins:seconds\
#SBATCH --mail-user=YOUREMAILHERE@email.com            # will receive an email when job starts, ends or fails\
#SBATCH --mail-type=BEGIN,END,FAIL                     # will receive an email when job starts, ends or fails\
#SBATCH -o "stdout.%j.%N"                              # standard out %j adds job number to outputfile name and %N adds the node name\
#SBATCH -e "stderr.%j.%N"                              # optional but it prints our standard error\
\
# ENTER COMMANDS HERE:\
\
module load java/1.8.0_121\
\
set -e # I think this means if anything fails it immediately exits and doesnt keep plowing ahead.\
\
#Written by Brian Bushnell\
#Last updated March 4, 2019\
\
#This script is designed to preprocess data for assembly of overlapping 2x150bp reads from Illumina HiSeq 2500.\
#Some numbers and steps may need adjustment for different data types or file paths.\
#For large genomes, tadpole and bbmerge (during the "Merge" phase) may need the flag "prefilter=2" to avoid running out of memory.\
#"prefilter" makes these take twice as long though so don't use it if you have enough memory.\
#The "rm SAMPLE_temp.fq.gz; ln -s reads.fq.gz SAMPLE_temp.fq.gz" is not necessary but added so that any pipeline stage can be easily disabled,\
#without affecting the input file name of the next stage.\
\
#interleave reads\
reformat.sh in=SAMPLE_R#.fastq.gz out=SAMPLE.fq.gz\
\
#Link the interleaved input file as "SAMPLE_temp.fq.gz"\
ln -s SAMPLE.fq.gz SAMPLE_temp.fq.gz\
\
# --- Preprocessing ---\
\
#Remove optical duplicates\
clumpify.sh in=SAMPLE_temp.fq.gz out=SAMPLE.clumped.fq.gz dedupe optical\
rm SAMPLE_temp.fq.gz; ln -s SAMPLE.clumped.fq.gz SAMPLE_temp.fq.gz\
\
#Remove low-quality regions\
filterbytile.sh in=SAMPLE_temp.fq.gz out=SAMPLE.filtered_by_tile.fq.gz\
rm SAMPLE_temp.fq.gz; ln -s SAMPLE.filtered_by_tile.fq.gz SAMPLE_temp.fq.gz\
\
#Trim adapters.  Optionally, reads with Ns can be discarded by adding "maxns=0" and reads with really low average quality can be discarded with "maq=8".\
bbduk.sh in=SAMPLE_temp.fq.gz out=SAMPLE.trimmed.fq.gz ktrim=r k=23 mink=11 hdist=1 tbo tpe minlen=70 ref=adapters ftm=5 ordered\
rm SAMPLE_temp.fq.gz; ln -s SAMPLE.trimmed.fq.gz SAMPLE_temp.fq.gz\
\
#Remove synthetic artifacts and spike-ins by kmer-matching.\
bbduk.sh in=SAMPLE_temp.fq.gz out=SAMPLE.filtered.fq.gz k=31 ref=artifacts,phix ordered cardinality\
rm SAMPLE_temp.fq.gz; ln -s SAMPLE.filtered.fq.gz SAMPLE_temp.fq.gz\
\
\
```\
\
  \
  \
* This script is just a template, I used it to create a SLURM script for each sample using a loop like this:  \
\
\
```\{bash, eval=FALSE\}\
\
# create a list of all samples:\
for x in *R1*gz;\
do\
echo "$\{x%_S*\}" >> samples.txt\
done\
\
\
# generate a SLURM script for each sample\
while read line\
do\
cat pre_mapping_QC.SLURM | sed "s/SAMPLE/$line/g" > "$line"_tmp.SLURM\
done < samples.txt\
\
# After you confirm they look good, submit with:\
for x in *_tmp.SLURM\
do\
sbatch $x\
done \
\
\
```\
\
\
### Map reads to Assembly  \
\
* this script is called metagenome_map4bin.SLURM\
\
```\{bash, eval=FALSE\}\
#!/bin/bash\
\
#SBATCH --job-name="SAMPLE"                            # name of the job submitted\
#SBATCH -p mem-low                                     # name of the queue you are submitting to\
#SBATCH -n 40                                          # number of cores/tasks in this job, you get all 20 cores with 2 threads per core with hyperthreading\
#SBATCH -N 1                                           # number of nodes\
#SBATCH --mem=200G                                     # memory allocation\
#SBATCH -t 2:00:00                                     # time allocated for this job hours:mins:seconds\
#SBATCH --mail-user=XXXXXXX            # will receive an email when job starts, ends or fails\
#SBATCH --mail-type=BEGIN,END,FAIL                     # will receive an email when job starts, ends or fails\
#SBATCH -o "stdout.%j.%N"                              # standard out %j adds job number to outputfile name and %N adds the node name\
#SBATCH -e "stderr.%j.%N"                              # optional but it prints our standard error\
\
# ENTER COMMANDS HERE:\
\
module load java/1.8.0_121\
module load samtools\
module load pigz\
\
set -e # I think this means if anything fails it immediately exits and doesnt keep plowing ahead.\
\
\
\
# FIRST GENERATING AN INDEXED REFERENCE WITH \
# bbmap.sh ref=metaspades_500_scaff.fa\
\
# then all jobs will just use this pre-indexed reference. I am doing this with an interactive shell prior to submitting the SLURMS\
\
# not specifying reference because of preindexing listed above.\
\
\
bbmap.sh in=../hiseq/SAMPLE.filtered.fq.gz outm=SAMPLE_mapped.sam.gz bs=SAMPLE.bamscript qtrim=10 untrim ambig=all pigz unpigz\
\
# this bamscript will generate sorted & indexed bam files\
chmod u+x SAMPLE.bamscript\
./SAMPLE.bamscript\
\
```\
\
\
* Again, this script is just a template, I used it to create a SLURM script for each sample using a loop like the one previously mentioned.  \
\
* This script will output sams as well as indexed bams for each sample mapping to the co-assembly.  \
\
### Run metabat  \
\
* This script is called metabat.SLURM and is available in the SLURM directory  \
\
\
```\{bash, eval=FALSE\}\
#!/bin/bash\
\
#SBATCH --job-name="metabat"                            # name of the job submitted\
#SBATCH -p mem                                          # name of the queue you are submitting to\
#SBATCH -n 40                                           # number of cores/tasks in this job, you get all 20 cores with 2 threads per core with hyperthreading\
#SBATCH -N 1                                            # number of nodes\
#SBATCH --mem=125G                                      # memory allocation\
#SBATCH -t 47:00:00                                     # time allocated for this job hours:mins:seconds\
#SBATCH --mail-user=YOUREMAIL@email.com                 # will receive an email when job starts, ends or fails\
#SBATCH --mail-type=BEGIN,END,FAIL                      # will receive an email when job starts, ends or fails\
#SBATCH -o "stdout.%j.%N"                               # standard out %j adds job number to outputfile name and %N adds the node name\
#SBATCH -e "stderr.%j.%N"                               # optional but it prints our standard error\
\
# ENTER COMMANDS HERE:\
\
module load metabat\
module load checkm/v1.0.11\
\
jgi_summarize_bam_contig_depths --outputDepth depth.txt --pairedContigs paired.txt --minContigLength 1000 --minContigDepth 2 *.bam\
\
metabat -i metaspades_500_scaff.fa -a depth.txt -o run1/bin -v --saveTNF saved.tnf --saveDistance saved.dist\
checkm lineage_wf -f run1/CheckM.txt -t 32 -x fa run1/ run1/SCG\
\
```\
\
* Metabat identified 1067 bins in our data, not all meet the requirements of iRep  \
* iRep requirements:  \
    1. Only considers contigs longer than 5kb  \
    2. more than 75% complete  \
    3. less than 2.5% contamination  \
    4. not more than 200 contigs\
  \
### Filter bins  \
\
* **remove contigs shorter than 5kb**  \
\
* This is the python code I used to remove contigs shorter than 5kb from each bin as well as remove the bins with more than 200 contigs.  \
  \
  \
  \
```\{python, eval=FALSE, python.reticulate=FALSE\}\
\
from Bio import SeqIO\
import os\
\
files = os.listdir()\
\
for fasta in files:\
        new_genome = []\
        genome = SeqIO.parse(fasta, 'fasta')\
        for seq in genome:\
                if len(seq.seq) > 4999:\
                        new_genome.append(seq)\
        if 0 < len(new_genome) < 200:\
                new_genome_handle = fasta.strip('.fa') + '_new.fasta'\
                SeqIO.write(new_genome, new_genome_handle, 'fasta')\
        else:\
                pass\
\
\
```\
\
\
### recalculate checkM completeness metrics  \
\
* This script runs checkM on the length filtered bins  \
* should be run from a directory containing the length filtered bin fastas  \
* Named 'checkm.SLURM' and is available in the SLURM directory  \
\
\
\
```\{bash, eval=FALSE\}\
#!/bin/bash\
\
#SBATCH --job-name="checkm"                            # name of the job submitted\
#SBATCH -p short                                       # name of the queue you are submitting to\
#SBATCH -n 16\
#SBATCH -N 1\
#SBATCH --mem=48G                                      # number of cores/tasks in this job, you get all 20 cores with 2 threads per core with hyperthreading\
#SBATCH -t 47:00:00                                    # time allocated for this job hours:mins:seconds\
#SBATCH --mail-user=XXXXXXX             # will receive an email when job starts, ends or fails\
#SBATCH --mail-type=BEGIN,END,FAIL                      # will receive an email when job starts, ends or fails\
#SBATCH -o "stdout.%j.%N"                               # standard out %j adds job number to outputfile name and %N adds the node name\
#SBATCH -e "stderr.%j.%N"                               # optional but it prints our standard error\
\
# ENTER COMMANDS HERE:\
\
module load checkm\
\
checkm lineage_wf -f out/CheckM.txt -t 16 -x fasta ./ ./out\
\
\
```\
\
* Now that the completness metrics are re-calculated, we need to pull out the bins that meet the requirements of iRep.  \
* This chunk identifies genomes that are more than 75% complete and have less than 2.5% contamination (column 13 and 14 in the checkm output).  \
* genomes meeting these requirements are then moved to their own 'good_bins' directory.  \
\
```\{bash, eval=FALSE\}\
awk '(NR>3) && ($13 > 75) && ($14 <2.5)' CheckM.txt > GOOD_BINS.txt\
\
for bin in $(awk '\{print $1\}' GOOD_BINS.txt) ; do cp "$bin".fa ./good_bins/; done\
\
```\
\
\
## Calculate growth rate for good bins  \
  \
* iRep is not installed system wide on CERES, but it can be easily installed using conda.  \
* Most of the work is done, we can actually use the sam files generated prior to metabat to calculate the coverage patterns of the good bins.  \
* The following script is a template for running iRep. It is named iREP7_template.SLURM and is available in the SLURM directory.  \
* You can generate SLURM scripts for each sample using the same kind of loop that was discussed earlier.  \
\
\
\
```\{bash, eval=FALSE\}\
#!/bin/bash\
\
#SBATCH --job-name="SAMPLE"                             # name of the job submitted\
#SBATCH -p short                                        # name of the queue you are submitting to\
#SBATCH -n 8                                            # number of cores/tasks \
#SBATCH -N 1\
#SBATCH --mem=100G                                      # memory allocation\
#SBATCH -t 47:00:00                                     # time allocated for this job hours:mins:seconds\
#SBATCH --mail-user=YOUREMAIL@email.com                 # will receive an email when job fails\
#SBATCH --mail-type=FAIL                                # will receive an email when job fails\
#SBATCH -o "stdout.%j.%N"                               # standard out %j adds job number to outputfile name and %N adds the node name\
#SBATCH -e "stderr.%j.%N"                               # optional but it prints our standard error\
\
# ENTER COMMANDS HERE:\
\
# activate the conda environment\
source ~/iREP/bin/activate\
\
gunzip SAMPLE_mapped.sam.gz\
\
# allows iRep to tolerate 7 mismatches, over ~150bp reads this is about 95%\
iRep -f *.fa -s SAMPLE_mapped.sam -o SAMPLE.out -mm 7 -t 8 --no-plot --sort\
\
\
\
```\
\
\
* Once all these jobs complete we will compile all the results into one file.  \
\
```\{bash, eval=FALSE\}\
#!/bin/bash\
\
#SBATCH --job-name="7_compile"                          # name of the job submitted\
#SBATCH -p short                                        # name of the queue you are submitting to\
#SBATCH -n 8                                            # number of cores/tasks in this job, you get all 20 cores with 2 threads per core with hyperthreading\
#SBATCH -N 1                                            # nodes\
#SBATCH --mem=32G                                       # memory allocation\
#SBATCH -t 47:00:00                                     # time allocated for this job hours:mins:seconds\
#SBATCH --mail-user=XXXXXXX             # will receive an email when job fails\
#SBATCH --mail-type=FAIL                                # will receive an email when job fails\
#SBATCH -o "stdout.%j.%N"                               # standard out %j adds job number to outputfile name and %N adds the node name\
#SBATCH -e "stderr.%j.%N"                               # optional but it prints our standard error\
\
# ENTER COMMANDS HERE:\
\
source ~/iREP/bin/activate\
\
iRep_filter.py -c 3 -w 93 -t *tsv --long > ALL_RESULTS_long\
\
\
```\
}